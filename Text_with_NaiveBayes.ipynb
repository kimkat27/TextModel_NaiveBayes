{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1Cy5L4Ikl_aBp2UBgVygKj9sBrBspc2b9","authorship_tag":"ABX9TyNmyHIkdH5R6GhEIUDe8bv0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","import os"],"metadata":{"id":"lzU-zhX2GcKS","executionInfo":{"status":"ok","timestamp":1738490384351,"user_tz":-480,"elapsed":274,"user":{"displayName":"KIMBERLY TRIPULCA","userId":"06264182523271799652"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from nltk import pos_tag\n","import re\n","\n","# Download NLTK resources\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('omw-1.4')"],"metadata":{"id":"YImFBug1weVg","executionInfo":{"status":"ok","timestamp":1738508645023,"user_tz":-480,"elapsed":607,"user":{"displayName":"KIMBERLY TRIPULCA","userId":"06264182523271799652"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a95eb9e3-193c-4d3d-d445-6bec9c533840"},"execution_count":193,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":193}]},{"cell_type":"code","source":["# Mount Google Drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PE-VZGSNwkxt","executionInfo":{"status":"ok","timestamp":1738490530298,"user_tz":-480,"elapsed":83607,"user":{"displayName":"KIMBERLY TRIPULCA","userId":"06264182523271799652"}},"outputId":"b4bf1cc0-6ffd-4101-e6a6-0bef74306e47"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# Dataset path\n","dataset_path = \"/content/drive/MyDrive/TextModel_NaiveBayes/Dataset/training.csv\"  # Adjust to your path"],"metadata":{"id":"2ZNVnC7Jwl4c","executionInfo":{"status":"ok","timestamp":1738511567139,"user_tz":-480,"elapsed":414,"user":{"displayName":"KIMBERLY TRIPULCA","userId":"06264182523271799652"}}},"execution_count":254,"outputs":[]},{"cell_type":"code","source":["# Check if the dataset exists\n","if os.path.exists(dataset_path):\n","    print(f\"Dataset is located at: {dataset_path}\")\n","else:\n","    print(\"Dataset folder not found.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XHSMLa6NwxAL","executionInfo":{"status":"ok","timestamp":1738511569085,"user_tz":-480,"elapsed":364,"user":{"displayName":"KIMBERLY TRIPULCA","userId":"06264182523271799652"}},"outputId":"23fa9a5d-b6cc-44d5-c3bf-b7161f75ef87"},"execution_count":255,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset is located at: /content/drive/MyDrive/TextModel_NaiveBayes/Dataset/training.csv\n"]}]},{"cell_type":"code","source":["# Load the dataset\n","df = pd.read_csv(dataset_path)"],"metadata":{"id":"R_6CljRxHl5u","executionInfo":{"status":"ok","timestamp":1738511719120,"user_tz":-480,"elapsed":302,"user":{"displayName":"KIMBERLY TRIPULCA","userId":"06264182523271799652"}}},"execution_count":266,"outputs":[]},{"cell_type":"code","source":["# Display the basic info of the dataset\n","df.info()\n","\n","num_characters = len(df)\n","num_words = len(df)\n","num_sentences = len(df)\n","\n","print(f\"Number of characters: {num_characters}\")\n","print(f\"Number of words: {num_words}\")\n","print(f\"Number of sentences: {num_sentences}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QqQjEmdEw8mq","executionInfo":{"status":"ok","timestamp":1738511720247,"user_tz":-480,"elapsed":7,"user":{"displayName":"KIMBERLY TRIPULCA","userId":"06264182523271799652"}},"outputId":"b0e16e29-ee3f-482e-bf81-68f37166b732"},"execution_count":267,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 16000 entries, 0 to 15999\n","Data columns (total 2 columns):\n"," #   Column  Non-Null Count  Dtype \n","---  ------  --------------  ----- \n"," 0   text    16000 non-null  object\n"," 1   label   16000 non-null  int64 \n","dtypes: int64(1), object(1)\n","memory usage: 250.1+ KB\n","Number of characters: 16000\n","Number of words: 16000\n","Number of sentences: 16000\n"]}]},{"cell_type":"code","source":["# Map numeric labels to their string names\n","label_mapping = {0: 'sadness', 1: 'joy', 2: 'love', 3: 'anger', 4: 'fear'}\n","unique_labels = df['label'].unique()\n","for label in unique_labels:\n","    print(f\"{label}: {label_mapping.get(label, 'Unknown label')}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KSS_ZhTvw-kb","executionInfo":{"status":"ok","timestamp":1738511723217,"user_tz":-480,"elapsed":243,"user":{"displayName":"KIMBERLY TRIPULCA","userId":"06264182523271799652"}},"outputId":"93b39db9-0670-4641-a12e-36a2a9416399"},"execution_count":268,"outputs":[{"output_type":"stream","name":"stdout","text":["0: sadness\n","3: anger\n","2: love\n","5: Unknown label\n","4: fear\n","1: joy\n"]}]},{"cell_type":"code","source":["# Checking for lowercase conversion\n","columns_checked = 0\n","columns_converted = 0\n","columns_not_converted = 0\n","\n","# Check for lowercase conversion in the text columns\n","for column in df.select_dtypes(include=['object']):\n","    columns_checked += 1\n","    non_lowercase_rows = df[~df[column].apply(lambda x: x.islower() if isinstance(x, str) else True)]\n","\n","    if not non_lowercase_rows.empty:\n","        columns_not_converted += 1\n","        print(f\"Column '{column}' has non-lowercase values:\")\n","        print(non_lowercase_rows[[column]])\n","    else:\n","        columns_converted += 1\n","        print(f\"Column '{column}' is already in lowercase.\")\n","\n","# Summary of column checks\n","print(\"\\nSummary:\")\n","print(f\"Total columns checked: {columns_checked}\")\n","print(f\"Columns already in lowercase: {columns_converted}\")\n","print(f\"Columns with non-lowercase values: {columns_not_converted}\")\n","\n","# Optionally, save rows with non-lowercase values\n","if columns_not_converted > 0:\n","    non_lowercase_rows.to_csv('non_lowercase_rows.csv', index=False)\n","    print(\"\\nRows with non-lowercase values have been saved to 'non_lowercase_rows.csv'.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y-98SLgVxDL4","executionInfo":{"status":"ok","timestamp":1738511724930,"user_tz":-480,"elapsed":375,"user":{"displayName":"KIMBERLY TRIPULCA","userId":"06264182523271799652"}},"outputId":"af9f2ed6-5fdf-4f8d-b477-362e9d31fb9b"},"execution_count":269,"outputs":[{"output_type":"stream","name":"stdout","text":["Column 'text' is already in lowercase.\n","\n","Summary:\n","Total columns checked: 1\n","Columns already in lowercase: 1\n","Columns with non-lowercase values: 0\n"]}]},{"cell_type":"code","source":["# Checking if the dataset is already tokenized\n","are_tokenized = df['text'].apply(lambda x: isinstance(x, list)).all()\n","\n","if are_tokenized:\n","    print(\"The dataset is tokenized.\")\n","else:\n","    print(\"The dataset is not tokenized.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S6fwBhTrxWYp","executionInfo":{"status":"ok","timestamp":1738511729540,"user_tz":-480,"elapsed":284,"user":{"displayName":"KIMBERLY TRIPULCA","userId":"06264182523271799652"}},"outputId":"d2962443-923c-4f4d-f919-c2924506cb2e"},"execution_count":270,"outputs":[{"output_type":"stream","name":"stdout","text":["The dataset is not tokenized.\n"]}]},{"cell_type":"code","source":["# Define unwanted terms\n","unwanted_terms = ['aa', 'aaaaaaand', 'aaaaand', 'aaaand', 'aac', 'aahhh', 'ab', 'abc']\n","unwanted_terms_pattern = r'\\b(' + '|'.join(map(re.escape, unwanted_terms)) + r')\\b'\n","\n","# Initialize lemmatizer and stopwords\n","lemmatizer = WordNetLemmatizer()\n","stop_words = set(stopwords.words('english'))\n","\n","# Function to preprocess text (tokenization and lemmatization without stop word removal)\n","def preprocess_text(text):\n","    # Tokenize the text\n","    tokens = word_tokenize(text)\n","\n","    # Remove unwanted terms based on the pattern\n","    cleaned_tokens = [\n","        word for word in tokens if not re.search(unwanted_terms_pattern, word.lower())\n","    ]\n","\n","     # Lemmatize the cleaned tokens\n","    lemmatized_tokens = [\n","        lemmatizer.lemmatize(word.lower())  # Lemmatize and convert to lowercase\n","        for word in cleaned_tokens if word.isalpha()  # Only keep alphabetic tokens\n","    ]\n","\n","    # Remove stop words after lemmatization\n","    final_tokens = [\n","        word for word in lemmatized_tokens if word not in stop_words\n","    ]\n","\n","    return final_tokens\n","\n","# Apply the preprocessing function to the 'text' column\n","df['processed_text'] = df['text'].apply(preprocess_text)\n","\n","# Display the first few rows to see the results\n","print(df[['text', 'processed_text']].head())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"6Tj3eBG-xf_B","executionInfo":{"status":"ok","timestamp":1738511734591,"user_tz":-480,"elapsed":3724,"user":{"displayName":"KIMBERLY TRIPULCA","userId":"06264182523271799652"}},"outputId":"bfb665f3-ecfc-4698-e67a-7cd866593917"},"execution_count":271,"outputs":[{"output_type":"stream","name":"stdout","text":["                                                text                                     processed_text\n","0                            i didnt feel humiliated                          [didnt, feel, humiliated]\n","1  i can go from feeling so hopeless to so damned...  [go, feeling, hopeless, damned, hopeful, aroun...\n","2   im grabbing a minute to post i feel greedy wrong  [im, grabbing, minute, post, feel, greedy, wrong]\n","3  i am ever feeling nostalgic about the fireplac...  [ever, feeling, nostalgic, fireplace, know, st...\n","4                               i am feeling grouchy                                 [feeling, grouchy]\n"]}]},{"cell_type":"code","source":["# Define the output file path\n","output_file_path = \"/content/drive/MyDrive/TextModel_NaiveBayes/Dataset/proccessed_text_dataOfTraining.csv\"\n","\n","# Save only the 'processed_text' column to a new CSV file\n","df[['processed_text']].to_csv(output_file_path, index=False)\n","\n","print(f\"Processed data has been saved to: {output_file_path}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Vge3JNNmQBB","executionInfo":{"status":"ok","timestamp":1738511738757,"user_tz":-480,"elapsed":522,"user":{"displayName":"KIMBERLY TRIPULCA","userId":"06264182523271799652"}},"outputId":"83a21450-3dca-4c52-c6a9-2c43f6151788"},"execution_count":272,"outputs":[{"output_type":"stream","name":"stdout","text":["Processed data has been saved to: /content/drive/MyDrive/TextModel_NaiveBayes/Dataset/proccessed_text_dataOfTraining.csv\n"]}]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer"],"metadata":{"id":"5NeoEFoeK5td","executionInfo":{"status":"ok","timestamp":1738507028469,"user_tz":-480,"elapsed":492,"user":{"displayName":"KIMBERLY TRIPULCA","userId":"06264182523271799652"}}},"execution_count":173,"outputs":[]},{"cell_type":"code","source":["# Dataset path\n","processedText_path = \"/content/drive/MyDrive/TextModel_NaiveBayes/Dataset/proccessed_text_dataOfTraining.csv\"  # Adjust to your path"],"metadata":{"id":"weKc3VlmVGFj","executionInfo":{"status":"ok","timestamp":1738511629822,"user_tz":-480,"elapsed":337,"user":{"displayName":"KIMBERLY TRIPULCA","userId":"06264182523271799652"}}},"execution_count":263,"outputs":[]},{"cell_type":"code","source":["# Load the dataset\n","df = pd.read_csv(processedText_path)\n","\n","# Step 2: Prepare the data (assuming 'processed_text' and 'label' are columns)\n","# Convert list of tokens to string, handling potential NaN values\n","X_train = df['processed_text'].apply(lambda x: ' '.join(eval(x)) if pd.notna(x) else '')\n","\n","# Step 3: Feature extraction using Bag of Words (BoW)\n","bow_vectorizer = CountVectorizer()  # No limit on features\n","X_train_bow = bow_vectorizer.fit_transform(X_train)\n","\n","# Step 3a: Count the unique features (terms) in the BoW vectorizer\n","num_features_bow = len(bow_vectorizer.get_feature_names_out())  # Number of unique terms in BoW\n","print(f\"Number of unique features (terms) using Bag of Words: {num_features_bow}\")\n","\n","# Step 3b: Display the actual features (terms) extracted by BoW\n","features = bow_vectorizer.get_feature_names_out()\n","print(f\"Unique features (terms) extracted by Bag of Words: \\n{features[:20]}\")  # Display the first 20 terms\n","\n","# Optionally, display the feature matrix (document-term matrix)\n","print(\"\\nDocument-Term Matrix (BoW representation):\")\n","print(X_train_bow.toarray())  # This will show the full document-term matrix (with counts)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rWlpfY55y2q4","executionInfo":{"status":"ok","timestamp":1738511745161,"user_tz":-480,"elapsed":1221,"user":{"displayName":"KIMBERLY TRIPULCA","userId":"06264182523271799652"}},"outputId":"7c54bfa8-f211-4e6a-9c69-6d76da769ed2"},"execution_count":273,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of unique features (terms) using Bag of Words: 13442\n","Unique features (terms) extracted by Bag of Words: \n","['aaron' 'abandon' 'abandoned' 'abandoning' 'abandonment' 'abated'\n"," 'abbigail' 'abdomen' 'abdominal' 'abducted' 'abelard' 'abhorrent' 'abide'\n"," 'ability' 'abit' 'able' 'ableness' 'abnormally' 'aboard' 'abominable']\n","\n","Document-Term Matrix (BoW representation):\n","[[0 0 0 ... 0 0 0]\n"," [0 0 0 ... 0 0 0]\n"," [0 0 0 ... 0 0 0]\n"," ...\n"," [0 0 0 ... 0 0 0]\n"," [0 0 0 ... 0 0 0]\n"," [0 0 0 ... 0 0 0]]\n"]}]},{"cell_type":"code","source":["# Step 4: Feature extraction using TF-IDF\n","tfidf_vectorizer = TfidfVectorizer()  # No limit on features\n","X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n","\n","# Step 4a: Count the unique features (terms) in the TF-IDF vectorizer\n","num_features_tfidf = len(tfidf_vectorizer.get_feature_names_out())  # Number of unique terms in TF-IDF\n","print(f\"Number of unique features (terms) using TF-IDF: {num_features_tfidf}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"968f4LjY0EQJ","executionInfo":{"status":"ok","timestamp":1738385226190,"user_tz":-480,"elapsed":657,"user":{"displayName":"KIMBERLY TRIPULCA","userId":"06264182523271799652"}},"outputId":"32da6b91-e08c-4eb9-8864-f53137e75d39"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of unique features (terms) using TF-IDF: 13462\n"]}]},{"cell_type":"code","source":["# Check the tokenized form of the text for a specific row\n","row_6469_tokens = eval(train_df['processed_text'][6469])\n","print(f\"Tokens for row 6469: {row_6469_tokens}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ix69BGHy7HrR","executionInfo":{"status":"ok","timestamp":1738386738930,"user_tz":-480,"elapsed":327,"user":{"displayName":"KIMBERLY TRIPULCA","userId":"06264182523271799652"}},"outputId":"3741f20b-38be-4399-c249-8b9b91243b99"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokens for row 6469: ['feel', 'place', 'posting', 'since', 'feel', 'hesitant', 'join', 'aa', 'full', 'force', 'could', 'use', 'insight', 'people', 'inside']\n"]}]}]}